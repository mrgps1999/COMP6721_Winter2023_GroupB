{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "EPOCH 0 | ITER 0 | AVG_LOSS 1.434604525566101\n",
      "EPOCH 0 | ITER 30 | AVG_LOSS 1.6141493320465088\n",
      "EPOCH 0 | ITER 60 | AVG_LOSS 1.6288108825683594\n",
      "EPOCH 0 | ITER 90 | AVG_LOSS 1.9926544427871704\n",
      "EPOCH 0 | ITER 120 | AVG_LOSS 1.6816366910934448\n",
      "EPOCH 0 | ITER 150 | AVG_LOSS 1.2351018190383911\n",
      "EPOCH 0 | ITER 180 | AVG_LOSS 1.6761395931243896\n",
      "EPOCH 0 | ITER 210 | AVG_LOSS 1.8917304277420044\n",
      "EPOCH 0 | ITER 240 | AVG_LOSS 1.5717209577560425\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "target_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "def make_dataset(dir):\n",
    "    items = []\n",
    "    labels = []\n",
    "\n",
    "    for rootdir, dirs, files in tqdm(os.walk(dir)):\n",
    "        #         print(rootdir)\n",
    "        for subdir in dirs:\n",
    "            #print(subdir)\n",
    "            for files1 in os.listdir(os.path.join(rootdir, subdir)):\n",
    "                image_path = os.path.join(os.path.join(rootdir, subdir), files1)\n",
    "                items.append(image_path)\n",
    "                labels.append(subdir)\n",
    "    print(len(items))\n",
    "    return items, labels\n",
    "\n",
    "def create_dataset(mode, dir, random_transform=None, target_transform=None):\n",
    "    images, labels = make_dataset(dir)\n",
    "    #print(images)\n",
    "    lenth = len(images)\n",
    "    labels = len(labels)\n",
    "    print(lenth)\n",
    "    print(labels)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_dataset(trainBatchsize, valBatchSize):\n",
    "    train_img_dir = r\"D:\\Animal_Classification\\dataset\\Animal\\train\"\n",
    "    val_img_dir = r\"D:\\Animal_Classification\\dataset\\Animal\\val\"\n",
    "\n",
    "#     training_Set = create_dataset('train', train_img_dir, random_transform=train_transform,\n",
    "#                                   target_transform=target_transform)\n",
    "#     validation_Set = create_dataset('val', val_img_dir, random_transform=train_transform,\n",
    "#                                     target_transform=target_transform)\n",
    "\n",
    "    training_Set = torchvision.datasets.ImageFolder(train_img_dir , transform=train_transform)\n",
    "    validation_Set = torchvision.datasets.ImageFolder(val_img_dir , transform=train_transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(training_Set, trainBatchsize, shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(training_Set, trainBatchsize, shuffle=False)\n",
    "    return trainloader, valloader\n",
    "\n",
    "def Model(inputsize):\n",
    "    model = models.mobilenet_v2(pretrained=False)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs,5)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epoch, step=50):\n",
    "    model.train()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #print(torch.cuda.is_available())\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        output = model(images)\n",
    "        loss = criterion(output,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%step==0:\n",
    "            print('EPOCH {} | ITER {} | AVG_LOSS {}'.format(epoch, i, loss))\n",
    "        writer.add_scalar('TRAIN_LOSS', loss, epoch)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = Model((3,224,224))\n",
    "    print(torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"using gpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    epochs = 30\n",
    "    train_batch_size = 1;\n",
    "    val_batch_size = 1;\n",
    "\n",
    "    train_loader, val_loader = load_dataset(train_batch_size, val_batch_size)\n",
    "    \n",
    "    #     history = open('./checkpoint/history.csv','w')\n",
    "    #     history.write('epochs,trainloss,valloss\\n')\n",
    "\n",
    "    for epoch in range(0,epochs):\n",
    "        print(epoch)\n",
    "        start = time.time()\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, epoch, step=30)\n",
    "        val_loss = train(model, val_loader, criterion, optimizer, epoch)\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print('EPOCH {} | LOSS {} | TIME {}'.format(epoch, train_loss, time.time() - start))\n",
    "        print('-' * 50)\n",
    "        print()\n",
    "\n",
    "\n",
    "#         history.write('{},{},{}\\n'.format(epoch, train_loss, validation_loss))\n",
    "#         save_checkpoint({'epoch': epoch,'state_dict': model.state_dict(),\n",
    "#             'optimizer' : optimizer.state_dict(),'loss' : train_loss}, './checkpoints/checkpoint_{}.ckpt'.format(epoch))\n",
    "#     history.close()\n",
    "\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# See PyCharm help at https://www.jetbrains.com/help/pycharm/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19924\\1965261682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnvidia\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msmi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
